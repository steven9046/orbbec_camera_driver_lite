/*****************************************************************************
 *                                                                            *
 *  Copyright (C) 2021 Steven Sun.                                        *
 *                                                                            *
 *  Licensed under the Apache License, Version 2.0 (the "License");           *
 *  you may not use this file except in compliance with the License.          *
 *  You may obtain a copy of the License at                                   *
 *                                                                            *
 *      http://www.apache.org/licenses/LICENSE-2.0                            *
 *                                                                            *
 *  Unless required by applicable law or agreed to in writing, software       *
 *  distributed under the License is distributed on an "AS IS" BASIS,         *
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  *
 *  See the License for the specific language governing permissions and       *
 *  limitations under the License.                                            *
 *                                                                            *
 *****************************************************************************/

#include <oni_camera.h>

using namespace openni;

OniCamera::OniCamera() {
  printf("Creating OniCamera instance.\n");
  // we can init something here
  oniEnvironmentInitialize();
}

OniCamera::~OniCamera() {}

bool OniCamera::oniEnvironmentInitialize() {
  oni_rc_ = OpenNI::initialize();
  printf("Oni environment initializing...\n");
  if (oni_rc_ != STATUS_OK) {
    printf("Oni environment initialize failed.\n");
    std::cout << OpenNI::getExtendedError() << std::endl;
    return false;
  }
  printf("Oni environment initialize success!\n");
  return true;
}

void OniCamera::openCamera() {
  printf("going to open oni camera.\n");
  OpenOniCamera(depth_uri_str_.c_str());
  getCameraParams();
  // point_cloud_ = boost::make_shared<pcl::PointCloud<pcl::PointXYZRGB>>();
}

void OniCamera::closeCamera() { printf("oni camera.\n"); }

// get camera params
bool OniCamera::getCameraParams() {
  int data_size = sizeof(cameraParams_);
  printf("data size : %d", data_size);
  memset(&cameraParams_, 0, sizeof(cameraParams_));
  Status rc = oni_device_.getProperty(openni::OBEXTENSION_ID_CAM_PARAMS, (uint8_t *)&cameraParams_, &data_size);
  if (!rc) {
    std::cout << "successfully get amera params:\n"
              << "k[0]: " << cameraParams_.l_k[0] << "\n"
              << "k[1]: " << cameraParams_.l_k[1] << "\n"
              << "k[2]: " << cameraParams_.l_k[2] << "\n"
              << "k[3]: " << cameraParams_.l_k[3] << "\n"
              << "k[4]: " << cameraParams_.l_k[4] << "\n"
              << "k[5]: " << cameraParams_.l_k[5] << std::endl;
    std::cout << "successfully get RGB camera params:\n"
              << "fx: " << cameraParams_.r_intr_p[0] << "\n"
              << "fy: " << cameraParams_.r_intr_p[1] << "\n"
              << "cx: " << cameraParams_.r_intr_p[2] << "\n"
              << "cy: " << cameraParams_.r_intr_p[3] << std::endl;
    std::cout << "successfully get RGB distortion params:\n"
              << "k1: " << cameraParams_.r_k[0] << "\n"
              << "k2: " << cameraParams_.r_k[1] << "\n"
              << "p1: " << cameraParams_.r_k[2] << "\n"
              << "p2: " << cameraParams_.r_k[3] << "\n"
              << "k3: " << cameraParams_.r_k[4] << std::endl;

    // 得到深度相机内参
    fdx_ = cameraParams_.l_intr_p[0] * ((float)(ONI_WIDTH) / RESOULTION_X);   // 320 / 640
    fdy_ = cameraParams_.l_intr_p[1] * ((float)(ONI_HEIGHT) / RESOULTION_Y);  // 200 /400
    u0_ = cameraParams_.l_intr_p[2] * ((float)(ONI_WIDTH) / RESOULTION_X);
    v0_ = cameraParams_.l_intr_p[3] * ((float)(ONI_HEIGHT) / RESOULTION_Y);

    // RGB相机内参
    rgb_fx_ = cameraParams_.r_intr_p[0] * ((float)(ORBBEC_RGB_WIDTH) / RESOULTION_X_RGB);
    rgb_fy_ = cameraParams_.r_intr_p[1] * ((float)(ORBBEC_RGB_HEIGHT) / RESOULTION_Y_RGB);
    rgb_cx_ = cameraParams_.r_intr_p[2] * ((float)(ORBBEC_RGB_WIDTH) / RESOULTION_X_RGB);
    rgb_cy_ = cameraParams_.r_intr_p[3] * ((float)(ORBBEC_RGB_HEIGHT) / RESOULTION_Y_RGB);
    
    // DEPTH_2_RGB
    mR << cameraParams_.r2l_r[0], cameraParams_.r2l_r[1], cameraParams_.r2l_r[2],
		cameraParams_.r2l_r[3], cameraParams_.r2l_r[4], cameraParams_.r2l_r[5],
		cameraParams_.r2l_r[6], cameraParams_.r2l_r[7], cameraParams_.r2l_r[8];

    mT << cameraParams_.r2l_t[0], cameraParams_.r2l_t[1], cameraParams_.r2l_t[2]; 

    // for (int i = 0; i < 4; i++) {
    //   for (int j = 0; j < 4; j++) {
    //     std::cout << " " << mRT(i, j);
    //   }
    //   std::cout << std::endl;
    // }

    return true;
  } else {
    std::cout << "get camera params failed" << std::endl;
    return false;
  }
}

/** steps to open an openni camera
 *  1. initialize openni
 *  2. assert a device and open
 *  3. assert a depth stream and start
 */
/** 1. must create IR stream first, or you can only get depth stream
 *  2. Ir stream rate is larger than Depth
 */
bool OniCamera::OpenOniCamera(const char *depth_uri) {
  // 1. connect to openni camera
  std::cout << "going to open: " << depth_uri << std::endl;
  printf("going to open: %s \n", depth_uri);
  std::string uri = depth_uri;
  oni_rc_ = oni_device_.open(depth_uri);
  if (oni_rc_ != STATUS_OK) {
    printf("Couldn't open orbbec device\n");
    // printf(OpenNI::getExtendedError());
    return false;
  }
  // 2.create and start depth stream
  if (oni_device_.getSensorInfo(SENSOR_DEPTH) != NULL) {
    oni_rc_ = oni_depth_stream_.create(oni_device_, SENSOR_DEPTH);
    if (oni_rc_ != STATUS_OK) {
      printf("Couldn't create depth stream\n");
      return false;
    }

    // 重新设置帧率
    oni_depthVideoMode_ = oni_depth_stream_.getVideoMode();
    oni_depthVideoMode_.setResolution(ONI_WIDTH, ONI_HEIGHT);
    oni_depthVideoMode_.setFps(ONI_FPS);
    printf("Setting resolution width: %d, height: %d \n", ONI_WIDTH, ONI_HEIGHT);
    printf("Setting FPS : %d \n", ONI_FPS);
    oni_depth_stream_.setVideoMode(oni_depthVideoMode_);

    oni_rc_ = oni_depth_stream_.start();
    if (oni_rc_ != STATUS_OK) {
      printf("Couldn't start depth stream\n");
      // TEMI_LOG(error) << OpenNI::getExtendedError();
      return false;
    }
  }

  // 2.create and start ir stream
  if (oni_device_.getSensorInfo(SENSOR_IR) != NULL) {
    oni_rc_ = oni_ir_stream_.create(oni_device_, SENSOR_IR);
    if (oni_rc_ != STATUS_OK) {
      printf("Couldn't create depth stream\n");
      // TEMI_LOG(error) << OpenNI::getExtendedError();
      return false;
    }

    // 重新设置帧率
    oni_irVideoMode_ = oni_ir_stream_.getVideoMode();
    oni_irVideoMode_.setResolution(ONI_WIDTH, ONI_HEIGHT);
    oni_irVideoMode_.setFps(ONI_FPS / 2);
    printf("Setting resolution width: %d, height: %d\n", ONI_WIDTH, ONI_HEIGHT);
    printf("Setting FPS : %d\n", ONI_FPS);
    oni_ir_stream_.setVideoMode(oni_irVideoMode_);
    // // If you wan't to start IR stream
    // oni_rc_ = oni_ir_stream_.start();
    // if(oni_rc_ != STATUS_OK)
    // {
    //   printf("Couldn't start depth stream\n");
    //   // TEMI_LOG(error) << OpenNI::getExtendedError();
    //   return false;
    // }
  }

  // if (oni_rc_ != STATUS_OK)
  // {
  //   ROS_ERROR("Couldn't start the depth stream");
  //   // ROS_ERROR(OpenNI::getExtendedError());
  //   return false;
  // }
  // point_cloud_ = boost::make_shared<pcl::PointCloud<pcl::PointXYZRGB>>();
  return true;
}

std::string OniCamera::enumerateDevices() {
  Array<DeviceInfo> deviceList;
  // TEMI_LOG(info) << "Going to enumerateDevices...";
  OpenNI::enumerateDevices(&deviceList);
  std::cout << deviceList.getSize() << "  devices are found" << std::endl;
  // ROS_INFO("％d evices are found", deviceList.getSize());
  // TEMI_LOG(info)<< deviceList.getSize() << " devices are found.";
  std::string depth_uri_;
  for (int i = 0; i != deviceList.getSize(); i++) {
    const openni::DeviceInfo &info = deviceList[i];
    std::string uri = info.getUri();
    // 下边这些属性都是一样的
    // Device Name: Astra
    // Device Vendor: Orbbec
    // Device Product ID: 1550
    // Device Vendor ID: 11205
    std::cout << "Device Name: " << info.getName() << std::endl;
    std::cout << "Device Vendor: " << info.getVendor() << std::endl;
    std::cout << "Device Product ID: " << info.getUsbProductId() << std::endl;
    std::cout << "Device Vendor ID: " << info.getUsbVendorId() << std::endl;
    std::cout << "Device URI: " << uri << std::endl;  // uri就是一个字符串
    if (uri.at(10) == camera_loc_) {
      printf("Catch camera!\n");
      // ROS_INFO("Device URI: %s", uri);//uri就是一个字符串
      std::cout << "Device URI:" << uri << std::endl;
      depth_uri_ = uri;  //.c_str();
      // ROS_INFO("depth_uri_: %s", depth_uri_);
    }
  }
  if (depth_uri_.empty()) {
    depth_uri_ = ANY_DEVICE;
  }
  return depth_uri_;
}

// set LDP on/off
void OniCamera::seOnitLDP(bool enable) {
  int data_size = 4;
  int enable_ = 1;
  if (enable == false) {
    enable_ = 0;
  }

  oni_depth_stream_.stop();
  oni_device_.setProperty(XN_MODULE_PROPERTY_LDP_ENABLE, (uint8_t *)&enable_, 4);
  oni_depth_stream_.start();
}

bool OniCamera::GetOniStreamData() {
  oni_depth_stream_.readFrame(&oni_depth_frame_);
  return true;
}

// void OniCamera::generatePointCloud(const cv::Mat &rgb_frame) {
//   // step1:init a blank point cloud whith colors.
//   static int seq = 0;
//   // clear and set point cloud
//   point_cloud_->clear();
//   point_cloud_->header.frame_id = "/base_link";
//   point_cloud_->is_dense = false;
//   // point_cloud_->header.stamp = ros::Time::now().toSec();
//   point_cloud_->header.seq = seq++;
//   pcl::PointXYZRGB point;
//   int invalid_close_pixels_count = 0;
//   int valid_points_counter = 0;
//   pcl::PointXYZRGB invalid_point;
//   invalid_point.x = invalid_point.y = invalid_point.z = std::numeric_limits<float>::quiet_NaN();
//   invalid_point.r = invalid_point.g = invalid_point.b = 0;  // black

//   // step3:generate point cloud
//   openni::DepthPixel *pDepth = (openni::DepthPixel *)oni_depth_frame_.getData();
//   float px = 0, py = 0, pz = 0;
//   static int frame_width = oni_depth_frame_.getWidth();
//   static int frame_height = oni_depth_frame_.getHeight();
//   static float fdx = cameraParams_.r_intr_p[0] * ((float)(frame_width) / RESOULTION_X);
//   static float fdy = cameraParams_.r_intr_p[1] * ((float)(frame_height) / RESOULTION_Y);
//   static float u0 = cameraParams_.r_intr_p[2] * ((float)(frame_width) / RESOULTION_X);
//   static float v0 = cameraParams_.r_intr_p[3] * ((float)(frame_height) / RESOULTION_Y);
//   // BinArray depth_histogram = { 0 };
//   // DistBinArray bin_depth_sum_mm = { 0.};
//   for (int j = 0; j < ONI_HEIGHT; j++) {           // rows -> j
//     for (int i = 0; i < ONI_WIDTH; i++) {          // cols ->i
//                                                    // if (i >= 80 && i <= 240 && j >= 50 && j <= 150)
//                                                    // {
//       uint16_t depth = pDepth[j * ONI_WIDTH + i];  // get depth from frame

//       if (depth < 200) {
//         invalid_close_pixels_count++;
//         point_cloud_->points.emplace_back(invalid_point);
//       } else {
//         //　这个函数把深度值转换为相机坐标系里的px,py,pz,相当于相机模型的作用
//         // oni_rc_ = oni_converter.convertDepthToWorld(oni_depth_stream_, i, j, pDepth[j * ONI_WIDTH + i], &px, &py, &pz);

//         // convert from pixel to camera
//         float tx = (i - u0) / fdx;
//         float ty = (j - v0) / fdy;
//         px = depth * tx * MM2M;
//         py = depth * ty * MM2M;
//         pz = depth * MM2M;
//         // 因为depth的tf和正常的相机tf不一样，是和baselink一样的，需要再转换一下
//         // 这里z(py)也要翻转一下，因为我们的相机是倒着安的
//         // tf::Vector3 point_vec = tf::Vector3(pz * 0.001, px * 0.001, py * 0.001);
//         // tf::Vector3 point_vec = tf::Vector3(px * 0.001, py * 0.001, pz * 0.001);

//         // if (point_vec.x() > 0.3 && point_vec.x() < 0.7)
//         // {
//         //   dist_1++;
//         // }
//         // else if (point_vec.x() > 1.3 && point_vec.x() < 1.7)
//         // {
//         //   dist_2++;
//         // }
//         // else if (point_vec.x() > 2.3 && point_vec.x() < 2.7)
//         // {
//         //   dist_3++;
//         // }
//         // avg_x += point_vec.x();
//         // valid_points_counter++;
//         // calculateDepthHistgram(point_vec.x(), depth_histogram, bin_depth_sum_mm);
//         // point.x = point_vec.x();
//         // point.y = point_vec.y();
//         // point.z = point_vec.z();
//         point.x = px;
//         point.y = py;
//         point.z = pz;
//         point.r = 255;
//         point.g = 255;
//         point.b = 255;
//         // step4.1:register point cloud with RGB(not activated now)
//         // #if RGB_REGISTERATION
//         //           // 对应的rgb中的点
//         //           cv::Vec2i rgb_pixel = d2r.at<cv::Vec2i>(j, i);
//         //           bool flag_1 = (0 <= i && i < 300);
//         //           bool flag_2 = (0 <= j && j < 300);

//         //           // TEMI_LOG(info) << "i :" << i << " in range 300?" << flag_1;
//         //           // TEMI_LOG(info) << "j :" << j << " in range 300?" << flag_2;
//         //           bool valid_index = (0 <= i && i < 300) && (0 <= j && j < 300);
//         //           // TEMI_LOG(info) << "depth_pixel in range(300x300)?" << valid_index;
//         //           if (valid_index) //如果depth点的索引在0-300之间(320X240,有超出范围的)
//         //           {
//         //             // TEMI_LOG(info) << "painting point ...";
//         //             // TEMI_LOG(info) << "rgb_pixel x:" << rgb_pixel[1] << " y: " << rgb_pixel[0];
//         //             if (rgb_pixel[0] == 0 && rgb_pixel[1] == 0) //depth point has no rgb point assigned,mark it as invalid.
//         //             {
//         //               invalid_close_pixels_count++;
//         //               point_cloud_->points.emplace_back(invalid_point);
//         //               continue;
//         //             }
//         //             else
//         //             {
//         //               int r = rgb_frame.at<cv::Vec3b>(rgb_pixel[0], rgb_pixel[1])[2];
//         //               int g = rgb_frame.at<cv::Vec3b>(rgb_pixel[0], rgb_pixel[1])[1];
//         //               int b = rgb_frame.at<cv::Vec3b>(rgb_pixel[0], rgb_pixel[1])[0];
//         //               // TEMI_LOG(info) << "r:" << r;
//         //               // TEMI_LOG(info) << "g:" << g;
//         //               // TEMI_LOG(info) << "b:" << b;
//         //               point.r = r;
//         //               point.g = g;
//         //               point.b = b;
//         //             }
//         //           }
//         //           else
//         //           {
//         //             // TEMI_LOG(info) << "point index out of 300, paint this point black. ";
//         //             invalid_close_pixels_count++;
//         //             point_cloud_->points.emplace_back(invalid_point);
//         //             continue;
//         //           }
//         // #endif
//         point_cloud_->points.emplace_back(point);
//       }
//       // } //end else
//     }  // innner loop
//   }    // outerloop
// }

void OniCamera::computeConvertedDepth(const cv::Mat &imDepth, cv::Mat &converted_depth){
    // RGB的像素坐标，像素值为单位为m的深度
    converted_depth = cv::Mat::zeros(ORBBEC_RGB_HEIGHT, ORBBEC_RGB_WIDTH, CV_32FC1);
    for(int i = 0; i < imDepth.cols; i ++){
        for(int j = 0; j < imDepth.rows; j ++){
            uint16_t depth = imDepth.at<uint16_t>(j, i);
            if(depth){
              float tx = (i - u0_) / fdx_;
              float ty = (j - v0_) / fdy_;
              float px = depth * tx ;
              float py = depth * ty;
              float pz = depth;
              Eigen::Matrix<float,3,1> pointDepth;
              pointDepth << px, py, pz;
              // std::cout << "Depth pixel: " << std::endl;
              // std::cout << pointDepth << std::endl;
              Eigen::Matrix<float,3,1> pointRGB;
              pointRGB = mR * pointDepth + mT;
              // std::cout << "  RGB pixel: " << std::endl;
              // std::cout << pointRGB << std::endl;

              // 投影到RGB相机
              float relative_x = static_cast<float>(pointRGB[0]) / static_cast<float>(pointRGB[2]);
              float relative_y = static_cast<float>(pointRGB[1]) / static_cast<float>(pointRGB[2]);
              // 像素坐标
              float rgb_pixel_x = relative_x * rgb_fx_ + rgb_cx_;
              float rgb_pixel_y = relative_y * rgb_fy_ + rgb_cy_;
              bool valid_index = (0 <= rgb_pixel_x && rgb_pixel_x < ORBBEC_RGB_WIDTH) && (0 <= rgb_pixel_y && rgb_pixel_y < ORBBEC_RGB_HEIGHT);
              if(valid_index){
                converted_depth.at<float>(rgb_pixel_y, rgb_pixel_x) = pointRGB[2] * 0.001;
              }
            }
        }
    }
    cv::Mat element = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(3, 3)); 
    cv::dilate(converted_depth,converted_depth,element);
    cv::flip(converted_depth, converted_depth, 1);
    // cv::namedWindow("converted_depth", 0);
    // cv::imshow("converted_depth", converted_depth);
    // cv::waitKey(1);
}